{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "156cb248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import svm\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "from scipy import signal\n",
    "import joblib\n",
    "from sklearn.feature_selection import SelectKBest,f_regression,chi2\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import smogn\n",
    "import pandas\n",
    "from scipy.spatial import distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a4108d",
   "metadata": {},
   "source": [
    "正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63be3635",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['週','工作天數','休假天數','前1週','前2週','前3週','前4週','每月第幾週','滑動平均',\n",
    "            '總下雨量','下雨天數','沒下雨天數']\n",
    "\n",
    "# need_normalize = ['週','工作天數','休假天數','前1週','前2週','前3週','前4週','數量','每月第幾週','總病人數','平均氣溫','前一週總病人數',\n",
    "#                   '前一週平均病人數','相似日','滑動平均','流感門診人數','流感急診人數','流感總人數','年','預測病人數量','平均濕度','隔週平均溫度差']\n",
    "need_normalize = ['週','工作天數','休假天數','前1週','前2週','前3週','前4週','數量','每月第幾週','相似日','滑動平均',\n",
    "                  '年','最大溫差','最大平均溫度差','相似資料','前1週人數','前1週最多人', '前1週平均人數','前1週開刀人數','前1週開刀平均人數','前1週開刀最多人']\n",
    "robust_need_normalize = ['週','前1週','前2週','前3週','前4週','數量','每月第幾週','平均氣溫','相似日','滑動平均','流感門診人數','流感急診人數','流感總人數']\n",
    "robust_need_normalize = ['週','前1週','前2週','前3週','前4週','數量','每月第幾週',]\n",
    "# need_normalize = ['週','年','工作天數','休假天數','數量','前1週','前2週','前3週','前4週']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58396790",
   "metadata": {},
   "source": [
    "最大最小值正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b279cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#正規化\n",
    "def data_normalize(total_data, need_normalize):\n",
    "    data = total_data.copy()\n",
    "    train_data = data[ (data['年']== 2019)|(data['年']== 2020)|(data['年']== 2021)]     \n",
    "#     train_data = data[data['年']== 2019] \n",
    "    #使用最大最小值進行標準化\n",
    "    for i in range(len(need_normalize)):\n",
    "        column = need_normalize[i]\n",
    "        molecular = data[column]-train_data[column].min()\n",
    "        denominator = train_data[column].max()-train_data[column].min()\n",
    "        data[column] = (molecular/denominator)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b5b0a9",
   "metadata": {},
   "source": [
    "Robust Scaling正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "848e2f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_normalize(total_data, need_normalize):\n",
    "    data = total_data.copy()\n",
    "    train_data = data[ (data['年']== 2019)|(data['年']== 2020)|(data['年']== 2021)]     \n",
    "#     train_data = data[data['年']== 2019] \n",
    "        #使用最大最小值進行標準化\n",
    "    for i in range(len(need_normalize)):\n",
    "        column = need_normalize[i]\n",
    "#         中位數\n",
    "        median = train_data[column].quantile(0.5)\n",
    "#         第1位數\n",
    "        first_quartile = train_data[column].quantile(0.25)\n",
    "#         第3位數\n",
    "        third_quartile = train_data[column].quantile(0.75)\n",
    "#         分子\n",
    "        molecular = data[column]-median\n",
    "#         分母\n",
    "        denominator = third_quartile - first_quartile\n",
    "        data[column] = (molecular/denominator)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272d1b8e",
   "metadata": {},
   "source": [
    "z-score正規化 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27cedcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize(total_data, need_normalize):\n",
    "    data = total_data.copy()\n",
    "#     train_data = data[ (data['年']== 2019)|(data['年']== 2020)]     \n",
    "    train_data = data[data['年']== 2019] \n",
    "        #使用最大最小值進行標準化\n",
    "    for i in range(len(need_normalize)):\n",
    "        column = need_normalize[i]\n",
    "#         分子\n",
    "        molecular = data[column]-train_data[column].mean()\n",
    "#         分母\n",
    "        denominator = train_data[column].std()\n",
    "        data[column] = (molecular/denominator)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13446e",
   "metadata": {},
   "source": [
    "重新計算工作日"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1a37aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def work_day(data,year):\n",
    "#     先刪除工作日和休假日\n",
    "    data = data.drop(['工作天數', '休假天數'], axis=1)\n",
    "    \n",
    "    check_date = pd.read_csv(f\"../使用量計算/週期資料.csv\",encoding='utf-8')\n",
    "    answer =  pd.DataFrame()\n",
    "    date =  pd.DataFrame()\n",
    "    date['日期'] = pd.DataFrame(pd.date_range(f'1/1/{year}',f'31/12/{year}'))\n",
    "    date['週'] = date['日期'].dt.isocalendar().week\n",
    "    date['年'] = date['日期'].dt.isocalendar().year\n",
    "    date = date.drop(date[date['週'] == 53].index).reset_index()\n",
    "    del_data =  date[date['週'] == 52].tail(1).index\n",
    "    date = date.drop(date[del_data.values[0]+1:].index).reset_index()\n",
    "    date[\"日期\"] = pd.to_datetime(date[\"日期\"] ,format='%Y/%m/%d')\n",
    "    check_date[\"date\"] = pd.to_datetime(check_date[\"date\"] ,format='%Y/%m/%d')\n",
    "    # 如果是工作日就存成true，否則存成false\n",
    "    date['休假日']=date['日期'].map(lambda x:(check_date['date']==x).any())\n",
    "    print(date)\n",
    "\n",
    "    for num in range(1, 53,1):\n",
    "        temp = date[date['週'] == num]\n",
    "#         計算該周工作日\n",
    "        holiday = len(temp[temp['休假日'] == True])\n",
    "        work_day = len(temp[temp['休假日'] == False])\n",
    "        date.loc[date['週']==num,'工作天數'] = work_day\n",
    "        date.loc[date['週']==num,'休假天數'] = holiday\n",
    "        if year == 2021 and num == 52:   \n",
    "            date.loc[date['週']==num,'休假天數'] = 2\n",
    "\n",
    "#     同一週期工作日、休假日都一樣所以只保留一筆\n",
    "    date.drop_duplicates(subset='週', keep='last', inplace=True)\n",
    "    date = date.drop(['日期', '休假日','level_0','index'], axis=1)\n",
    "  \n",
    "#     answer = pd.merge(data, date, on='週',how='outer')\n",
    "\n",
    "    answer = pd.merge(data, date, on='週')\n",
    "#     answer = answer.sort_values(['帳務日期'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "#     刪除日期和工作日\n",
    "    answer.rename(columns={'年_x': '年'}, inplace=True)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e734ae",
   "metadata": {},
   "source": [
    "每週資料加總"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d666476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def week_data_sum(data):\n",
    "#     刪除第53週\n",
    "    data = data.drop(data[data['週'] == 53].index)\n",
    "    part_compute = pd.DataFrame()\n",
    "    total_sum = 0\n",
    "    for num in range(1, 53,1):\n",
    "        temp = data[data['週'] == num]\n",
    "        last_temp = temp.tail(1)\n",
    "        if len(temp) == 0:\n",
    "            \n",
    "            last_temp =  data.tail(1)\n",
    "            last_temp['數量'] = 0\n",
    "            last_temp['週'] = num\n",
    "            print(\"沒有:\",last_temp)\n",
    "            part_compute = part_compute.append(last_temp)\n",
    "        else:\n",
    "            temp = temp.drop(temp[temp['數量'] < 0].index)\n",
    "            last_temp['數量'] =  temp['數量'].sum()\n",
    "            part_compute = part_compute.append(last_temp)\n",
    "        total_sum = total_sum +part_compute['數量'].values[0]\n",
    "    print(part_compute)\n",
    "    print(total_sum)\n",
    "   \n",
    "    \n",
    "    return part_compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923f4766",
   "metadata": {},
   "source": [
    "提取前n天的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "828331cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 總資料data\n",
    "# 要提取前幾天資料n_day\n",
    "def take_data(data,n_day):\n",
    "    data_copy = data.copy()\n",
    "    # 總共要提取幾次\n",
    "    for num in range(n_day, len(data_copy),1):\n",
    "#         每次提取n筆\n",
    "        for time in range(1,n_day+1,1):\n",
    "            before_day = data_copy.loc[num-time:num-time,\"數量\"].round(decimals = 2)\n",
    "            print(type(before_day))\n",
    "            data_copy.loc[num:num,f'前{time}週'] = before_day.values[0]\n",
    "\n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e054af0",
   "metadata": {},
   "source": [
    "判斷第幾週"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "171a3f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_week(week):\n",
    "    if week == 0:\n",
    "        return '第4週'\n",
    "    elif week == 1:\n",
    "        return '第1週'\n",
    "    elif week == 2:\n",
    "        return '第2週'\n",
    "    else:\n",
    "        return '第3週'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121e6ead",
   "metadata": {},
   "source": [
    "新增降水量資料&是否有下雨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f092a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_data(data):\n",
    "    # 讀取降水資料\n",
    "    precipitation = pd.read_csv(f\"../使用量計算/天氣資料/斗六降水量資料1.csv\")\n",
    "    #     為了更使用量資料合併更改欄位名稱\n",
    "    precipitation.rename(columns={'日期': '帳務日期'}, inplace=True)\n",
    "    precipitation['帳務日期'] =  pd.to_datetime(precipitation['帳務日期'])\n",
    "    data['帳務日期'] =  pd.to_datetime(data['帳務日期'])\n",
    "    print(precipitation.head())\n",
    "    print(\"合併前長度:\",len(data))\n",
    "    columns = [\"年\",\"週\"]\n",
    "    precipitation.drop_duplicates(subset=columns, keep='last', inplace=True)\n",
    "    data = pd.merge(data, precipitation, on=columns)\n",
    "    print(\"合併後長度:\",len(data))\n",
    "    data = data[data.columns.drop(list(data.filter(regex='週_y')))]\n",
    "    data = data[data.columns.drop(list(data.filter(regex='年_y')))]\n",
    "    data.rename(columns={'週_x': '週'}, inplace=True)\n",
    "    data.rename(columns={'年_x': '年'}, inplace=True)\n",
    "    data.rename(columns={'帳務日期_x': '帳務日期'}, inplace=True)\n",
    "    return data\n",
    "    #     return precipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab9bcfc",
   "metadata": {},
   "source": [
    "整理每周開刀人數、平均開刀人數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a054653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def surgery_data(data):\n",
    "    people = pd.read_csv(f\"../使用量計算/醫院提供病房人數資料/五病房開刀人數.csv\")\n",
    "    people = people[people.columns.drop(list(people.filter(regex='Unnamed')))]\n",
    "    people.rename(columns={'手術日期': '帳務日期'}, inplace=True)\n",
    "    people['帳務日期'] =  pd.to_datetime(people['帳務日期'])\n",
    "    people['週'] = people['帳務日期'].dt.isocalendar().week\n",
    "    people['年'] = people['帳務日期'].dt.isocalendar().year\n",
    "    columns = ['週','年']\n",
    "    print(\"合併前長度:\",len(data))\n",
    "    data = pd.merge(data, people, on=columns)\n",
    "    print(\"合併後長度:\",len(data))\n",
    "    print(data)\n",
    "    data.rename(columns={'週_x': '週'}, inplace=True)\n",
    "    data.rename(columns={'年_x': '年'}, inplace=True)\n",
    "    data.rename(columns={'帳務日期_x': '帳務日期'}, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13792c19",
   "metadata": {},
   "source": [
    "整理每周總病人人數、平均病人人數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de1c5c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def people_data(data):\n",
    "    people = pd.read_csv(f\"../使用量計算/醫院提供病房人數資料/五病房人數.csv\")\n",
    "    people = people[people.columns.drop(list(people.filter(regex='Unnamed')))]\n",
    "    people.rename(columns={'住院日期': '帳務日期'}, inplace=True)\n",
    "    people['帳務日期'] =  pd.to_datetime(people['帳務日期'])\n",
    "    people['週'] = people['帳務日期'].dt.isocalendar().week\n",
    "    people['年'] = people['帳務日期'].dt.isocalendar().year\n",
    "    columns = ['週','年']\n",
    "    print(\"合併前長度:\",len(data))\n",
    "    data = pd.merge(data, people, on=columns)\n",
    "    print(\"合併後長度:\",len(data))\n",
    "    print(data)\n",
    "    data.rename(columns={'週_x': '週'}, inplace=True)\n",
    "    data.rename(columns={'年_x': '年'}, inplace=True)\n",
    "    data.rename(columns={'帳務日期_x': '帳務日期'}, inplace=True)\n",
    "    return data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf9a6c2",
   "metadata": {},
   "source": [
    "計算相關性，並返回相關衛材的前一個禮拜使用量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb5411a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_corr(part_no,take_num):\n",
    "    corr_data = pd.DataFrame()\n",
    "    final = pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(f\"../使用量計算/衛材相關性計算/五病房/\"):\n",
    "        if filename=='.ipynb_checkpoints':\n",
    "            continue\n",
    "        temp = pd.read_csv(f'../使用量計算/衛材相關性計算/五病房/{filename}')\n",
    "        name = temp['料號'].values[0] \n",
    "        temp[name] = temp['數量']\n",
    "        corr_data = pd.concat([corr_data, temp[name] ], axis = 1)\n",
    "   \n",
    "    print(\"全部資料\",corr_data)\n",
    "    corr = corr_data.corr()\n",
    "    print(\"相關數值\",corr[part_no])\n",
    "#     排序取出前3筆最大值\n",
    "    data_sort=corr[part_no].sort_values(ascending=False)\n",
    "    corr_part_no = data_sort.head(take_num+1).index\n",
    "#     對3個衛材進行前一週使用量提取\n",
    "    for num in range(1,take_num+1,1):\n",
    "        temp = pd.read_csv(f'../使用量計算/衛材相關性計算/五病房/{corr_part_no[num]}.csv')\n",
    "#         提取相關的衛材前一週資料\n",
    "        before_temp = take_data(temp,1)\n",
    "\n",
    "        before_temp[corr_part_no[num]] = before_temp['前1週']\n",
    "        final = pd.concat([final, before_temp[corr_part_no[num]]], axis = 1)\n",
    "#     print(\"final長度:\",len(final))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e816eb16",
   "metadata": {},
   "source": [
    "數據平滑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6b2805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(data,smooth_num):\n",
    "    data['前1週'] = signal.savgol_filter(data['前1週'], len(data), smooth_num )\n",
    "    data['前2週'] = signal.savgol_filter(data['前2週'], len(data), smooth_num )\n",
    "    data['前3週'] = signal.savgol_filter(data['前3週'], len(data), smooth_num )\n",
    "    data['前4週'] = signal.savgol_filter(data['前4週'], len(data), smooth_num )\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c69ae",
   "metadata": {},
   "source": [
    "提取工作日休假日相似日的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25e3ac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_day(data):\n",
    "#     第一筆不會有相似日，因此從1開始\n",
    "    for i in range(1,len(data),1):\n",
    "#         找到index前幾筆資料\n",
    "        temp = data.iloc[0:i]\n",
    "#       提取查詢日的工作天數、休假天數\n",
    "        work_day = data.loc[i:i,\"工作天數\"].values[0]\n",
    "        qk_day = data.loc[i:i,\"休假天數\"].values[0]\n",
    "#      目標日\n",
    "        target = temp[(temp[\"工作天數\"] == work_day) & (temp[\"休假天數\"] == qk_day)]\n",
    "#         如果找不到相似日則以前一週使用量代替\n",
    "        if len(target) == 0:\n",
    "             data.loc[i:i,\"相似日\"] = data.loc[i-1:i-1,'數量'].values[0]\n",
    "        else:\n",
    "             data.loc[i:i,\"相似日\"] = target.iloc[-1]['數量']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cf1be2",
   "metadata": {},
   "source": [
    "判斷過年以及替換值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a8d6a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019年過年 2月2日~2月10日\n",
    "# 2020年過年 1月23日~1月29日\n",
    "# 2021年過年 2月10日~2月16日\n",
    "# 2022年過年 1月31日~2月6日\n",
    "# 2021年2月20日雖然為假日但為補班日\n",
    "# 先計算上述時間是第幾週，並判斷該週休假日是否大於等於4天\n",
    "def new_year(data):\n",
    "    date = pd.DataFrame()\n",
    "    new_year = [[2019,2,2,2019,2,10],[2020,1,23,2020,1,29],[2021,2,10,2021,2,16],[2022,1,31,2022,2,6]]\n",
    "    for i in range(len(new_year)):\n",
    "        date = pd.DataFrame()\n",
    "        date['日期'] = pd.DataFrame(pd.date_range(f'{new_year[i][1]}/{new_year[i][2]}/{new_year[i][0]}',f'{new_year[i][4]}/{new_year[i][5]}/{new_year[i][3]}'))\n",
    "        date['週'] = date['日期'].dt.isocalendar().week\n",
    "        date.drop_duplicates(subset='週', keep='last', inplace=True)\n",
    "       \n",
    "    #     選出該週並判斷該週的休假日是否大於等於4\n",
    "        for week in range(len(date)):\n",
    "            target = data[(data[\"年\"] == new_year[i][0]) & (data[\"週\"] == date['週'].values[week])]\n",
    "            target_index = data[(data[\"年\"] == new_year[i][0]) & (data[\"週\"] == date['週'].values[week])].index.astype(int)\n",
    "#             如果沒有那週的資料則用去年的資料補上並做更改\n",
    "            if target.empty:\n",
    "                temp = data[(data[\"年\"] == new_year[i][0]-1) & (data[\"週\"] == date['週'].values[week])]\n",
    "                temp['年'] = 2022\n",
    "                temp['數量'] = 0\n",
    "                temp['休假天數'] = 7\n",
    "                temp['工作天數'] = 0\n",
    "                data = data.append(temp,ignore_index=True)\n",
    "                target = data[(data[\"年\"] == new_year[i][0]) & (data[\"週\"] == date['週'].values[week])]\n",
    "                target_index = data[(data[\"年\"] == new_year[i][0]) & (data[\"週\"] == date['週'].values[week])].index.astype(int)\n",
    "\n",
    "#                 找出2019年第五周的資料並將數量改成0，年改成2022\n",
    "                \n",
    "            if target['休假天數'].values[0] >= 4:\n",
    "                data.loc[target_index.values[0]:target_index.values[0],'過年'] = True\n",
    "#         找出有過年的那幾週的資料\n",
    "    data = data[data.columns.drop(list(data.filter(regex='level_0')))]\n",
    "    new_year_day = data[data['過年'] == True].reset_index()\n",
    "#     print('-----------------------------',new_year_day)\n",
    "  \n",
    "\n",
    "#     找出同一年重複的，只留最少的\n",
    "    for i in range(len(new_year)):\n",
    "        if len(new_year_day[new_year_day['年'] == new_year[i][0]]) > 1:\n",
    "#             找出不要的\n",
    "            temp1 = data[(data['年']== new_year[i][0]) & (data['過年']==True)]\n",
    "            temp1_index = temp1[temp1['數量'] == temp1['數量'].max()].index.values[0]\n",
    "            data.loc[temp1_index:temp1_index,\"過年\"] = False\n",
    "            temp2= new_year_day[new_year_day['年']== new_year[i][0]]\n",
    "            temp2_index =temp2[temp2['數量'] == temp2['數量'].max()].index.values[0]\n",
    "            new_year_day = new_year_day.drop(temp2_index)\n",
    "    \n",
    "#     新增使用量欄位\n",
    "    new_year_day = data[data['過年'] == True].reset_index()\n",
    "    new_year_day_index = data[data['過年'] == True].index.astype(int)\n",
    "    for i in range(len(new_year_day)):\n",
    "        if i == 0:\n",
    "            before_day = data.loc[new_year_day_index[i]-1:new_year_day_index[i]-1,\"數量\"].round(decimals = 2)\n",
    "        else:\n",
    "            before_day = new_year_day.loc[i-1:i-1,'數量'].round(decimals = 2)\n",
    "    #         data.loc[new_year_day_index[i]:new_year_day_index[i],f'使用量(過年)'] = before_day.values[0]\n",
    "        new_year_day.loc[i:i,'使用量(過年)'] = before_day.values[0]\n",
    "#     刪除過年資料並且另外處理計算\n",
    "    data = data.drop(data[data['過年']==True].index).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    return data,new_year_day\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461e25a7",
   "metadata": {},
   "source": [
    "計算前N筆的平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb4d423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def before_n_mean(data,window_size):\n",
    "    total_data = data.reset_index(drop=True).copy()\n",
    "    # 計算滑動窗口\n",
    "    for num in range(window_size, len(total_data),1):\n",
    "        print(num)\n",
    "    #     print(num-window_size)\n",
    "        average = total_data.loc[num-window_size:num-1,\"數量\"].mean().round(decimals = 2)\n",
    "    #     print(average.round(decimals = 2))\n",
    "#         standard_deviation = total_data.loc[num-window_size:num-1,\"數量\"].std().round(decimals = 2)\n",
    "#         variation = total_data.loc[num-window_size:num-1,\"數量\"].var().round(decimals = 2)\n",
    "        total_data.loc[num:num,'滑動平均'] = average\n",
    "#         total_data.loc[num:num,'滑動標準差'] = standard_deviation\n",
    "#         total_data.loc[num:num,'滑動變異數'] = variation\n",
    "    return total_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf28ef",
   "metadata": {},
   "source": [
    "計算特徵的分數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42224817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_point(features,x_train,target,n):\n",
    "    columns = []\n",
    "    selector = SelectKBest(f_regression,k=len(features))\n",
    "    selector.fit(x_train[features],target)\n",
    "#     將計算後的P值轉為分數\n",
    "    scores = -np.log10(selector.pvalues_)\n",
    "#     scores = selector.pvalues_\n",
    "#     印出答案&降冪排序\n",
    "    print(\"重要特徵排序:\")\n",
    "    indices = np.argsort(scores)[::-1]\n",
    "    for f in range(len(scores)):\n",
    "        print(\"%0.2f %s\" % (scores[indices[f]],features[indices[f]]))\n",
    "#     回傳前n比當作columns\n",
    "    for f in range(0,n,1):\n",
    "       columns.append(features[indices[f]])\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aa0a54",
   "metadata": {},
   "source": [
    "K-means分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c49f3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_mean(total_data):\n",
    "    k_mean = pd.DataFrame()\n",
    "        # k = 1~9 做9次kmeans, 並將每次結果的inertia收集在一個list裡\n",
    "    kmeans_list = [KMeans(n_clusters=k, random_state=46).fit(total_data)\n",
    "                    for k in range(1, 13,1)]\n",
    "    silhouette_scores = [silhouette_score(total_data, model.labels_)\n",
    "                     for model in kmeans_list[1:]]\n",
    "    print(\"輪廓係數\",silhouette_scores)\n",
    "#     擬合好模型後我們可以計輪廓係數，用來評估集群的成效，其 silhouette_scores 越大代表越好。\n",
    "#     找出最大值的索引位置\n",
    "    index = silhouette_scores.index(max(silhouette_scores))\n",
    "    index = index + 2\n",
    "    print(index)\n",
    "\n",
    "    temp=KMeans(n_clusters=index, random_state=46).fit_predict(total_data)\n",
    "    print(temp)\n",
    "\n",
    "    k_mean[\"資料分類\"] = temp+1\n",
    "    return k_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e18fafa",
   "metadata": {},
   "source": [
    "流感人數計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b1a59f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theflu_sum(total_data):\n",
    "    concat_data = pd.DataFrame()\n",
    "    clinic=pd.read_csv(f\"../爬蟲/流感門診人數.csv\")\n",
    "    emergency=pd.read_csv(f\"../爬蟲/流感急診人數.csv\")\n",
    "    clinic = clinic.rename(columns={\"就診人次\": \"流感門診人數\"})\n",
    "    emergency = emergency.rename(columns={\"就診人次\": \"流感急診人數\"})\n",
    "    columns = ['年','週']\n",
    "    concat_data = pd.merge(clinic, emergency, on=columns)\n",
    "    concat_data = concat_data[['年','週','流感門診人數','流感急診人數']]\n",
    "    concat_data['流感總人數'] = concat_data[['流感門診人數','流感急診人數']].sum(1)\n",
    "    total_data = pd.merge(total_data, concat_data, on=columns)\n",
    "#     print(\"就診人數\",total_data.head())\n",
    "#     print(total_data.columns)\n",
    "    return total_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2defdb39",
   "metadata": {},
   "source": [
    "合併預測的病房人數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c4f84e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_people(total_data):\n",
    "    people=pd.read_csv(f\"../預測病人人數/arima五病房人數預測.csv\")\n",
    "    columns = ['年','週']\n",
    "    concat_data = pd.merge(total_data, people, on=columns)\n",
    "    print(len(total_data))\n",
    "    print(\"合併後資料\",concat_data.head(100))\n",
    "    return concat_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad40d6d",
   "metadata": {},
   "source": [
    "SMOGN擴增資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24287f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMOGN(total_train):\n",
    "    print(\"擴增前:\",total_train)\n",
    "    train_smogn = smogn.smoter(\n",
    "\n",
    "         ## main arguments\n",
    "        data = total_train,           ## pandas dataframe\n",
    "        y = '數量',          ## string ('header name')\n",
    "        k = 7,                    ## positive integer (k < n)\n",
    "        samp_method = 'extreme',  ## string ('balance' or 'extreme')\n",
    "#         random_seed = 1,\n",
    "      \n",
    "        ## phi relevance arguments\n",
    "        rel_thres = 0.80,         ## positive real number (0 < R < 1)\n",
    "        rel_method = 'auto',      ## string ('auto' or 'manual')\n",
    "        rel_xtrm_type = 'high',   ## string ('low' or 'both' or 'high')\n",
    "        rel_coef = 0.5           ## positive real number (0 < R)\n",
    "        \n",
    "    )\n",
    "#     print(\"擴增後的資料:\",train_smogn)\n",
    "    return train_smogn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76593d1c",
   "metadata": {},
   "source": [
    "計算平均溫度差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3a3cabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Temperature_difference(total_data):\n",
    "\n",
    "    for i in range(len(total_data)):\n",
    "        if i != 0: \n",
    "            temp = total_data.loc[i:i,'平均氣溫'].values[0]\n",
    "            pre_temp = total_data.loc[i-1:i-1,'平均氣溫'].values[0]\n",
    "            total_data.loc[i:i,\"隔週平均溫度差\"] = abs(temp - pre_temp)\n",
    "            \n",
    "        else:\n",
    "            total_data.loc[i:i,\"隔週平均溫度差\"] = 0\n",
    "    return total_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85dbfa2",
   "metadata": {},
   "source": [
    "每一週最大溫差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2113db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaxTemperature_difference(data):\n",
    "    total_data = pd.DataFrame()\n",
    "#     讀取溫度差資料\n",
    "    Temperature=pd.read_csv(f\"../使用量計算/天氣資料/2022溫度差資料.csv\")\n",
    "    \n",
    "#     刪除第53週\n",
    "#     刪除第53週\n",
    "    Temperature = Temperature.drop(Temperature[Temperature['週'] == 53].index)\n",
    "#     過濾資料，只保留每週最大值\n",
    "    Temperature = Temperature.sort_values('最大溫差', ascending=False).drop_duplicates(subset=['年', '週'], keep='first')\n",
    "    Temperature = Temperature.sort_values(['年', '週'], ascending=True)\n",
    "#     刪除不必要的欄位\n",
    "    Temperature = Temperature[Temperature.columns.drop(list(Temperature.filter(regex='Unnamed')))].reset_index(drop=True)\n",
    "#     合併資料\n",
    "    columns = ['年', '週']\n",
    "    total_data = pd.merge(data, Temperature, on=columns)\n",
    "    print(\"溫差\",total_data)\n",
    "    return total_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a512b8",
   "metadata": {},
   "source": [
    "3個標準差異常排除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c89a0ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_outlier(total_data,train_data):\n",
    "    mean = train_data[\"數量\"].mean()\n",
    "    std = train_data[\"數量\"].std()\n",
    "#     if std < 1:\n",
    "#         Threshold = 9\n",
    "#     else:\n",
    "#         Threshold = 3\n",
    "    Threshold = 2\n",
    "#     移除前長度\n",
    "    before_len = len(total_data)\n",
    "    total_data['zscore'] = ( total_data[\"數量\"] - mean ) / std\n",
    "    total_data = total_data[(total_data['zscore']<Threshold) & (total_data['zscore']>(-1*Threshold))]\n",
    "#     移除後長度\n",
    "    after_len = len(total_data)\n",
    "  \n",
    "    print(\"平均值\",mean)\n",
    "    print(\"標準差\",std)\n",
    "    print(\"長度\",len(total_data))\n",
    "    \n",
    "    return total_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95510a9f",
   "metadata": {},
   "source": [
    "1.5倍4分衛距異常值移除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21ecc672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Iqr_outlier(total_data,train_data):\n",
    "    data = total_data.copy()\n",
    "    \n",
    "    \n",
    "    re_before = len(data)\n",
    "    n=1.5\n",
    "    Q3 = np.percentile(train_data['數量'],75) \n",
    "    Q1 = np.percentile(train_data['數量'],25)\n",
    "    #IQR = Q3-Q1\n",
    "    IQR = Q3 - Q1 \n",
    "    \n",
    "    #outlier step\n",
    "    outlier_step = n * IQR\n",
    "    dq3 = data[~(data['數量'] < Q3 + outlier_step)]\n",
    "    dq1 = data[~(data['數量'] > Q1 - outlier_step)]\n",
    "    \n",
    "    outlier = pd.concat([dq3, dq1])\n",
    "\n",
    "\n",
    "    #outlier = Q3 + n*IQR \n",
    "    data=data[data['數量'] < Q3 + outlier_step]\n",
    "    #outlier = Q1 - n*IQR \n",
    "    data=data[data['數量'] > Q1 - outlier_step]\n",
    "\n",
    "    \n",
    "    re_after = len(data)\n",
    "\n",
    "    print(f'移除前: {re_before}, 移除後: {re_after}, 共移除 {re_before-re_after} 筆') \n",
    "    print(f'IQR: {IQR}, Q3: {Q3}, Q1: {Q1}, outlier(1.5*IQR): {outlier_step}, Q3+outlier: { Q3 + outlier_step}, Q1-outlier: {Q1 - outlier_step}')\n",
    "    remove_len = re_before-re_after\n",
    "    \n",
    "#     return data, outlier\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e5c128",
   "metadata": {},
   "source": [
    "算出相似資料的平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5188e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_count_distance(data):\n",
    "    total_data = data.copy()\n",
    "    temp = total_data[0:len(total_data)-1]\n",
    "    temp = temp[['前1週','前2週','前3週','工作天數','最大溫差']]\n",
    "    find_value = pd.DataFrame(distance_matrix(temp.values, temp.values))\n",
    "    answer = find_value[0].sum()/len(total_data)-1\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86e867ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_count_distance(data):\n",
    "    total_data = data.copy()\n",
    "    temp = total_data[0:len(total_data)-1]\n",
    "    temp = temp[['前1週','前2週','前3週','工作天數','最大溫差']]\n",
    "    find_value = pd.DataFrame(distance_matrix(temp.values, temp.values))\n",
    "    answer = find_value[0].sum()/len(total_data)-1\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c8580",
   "metadata": {},
   "source": [
    "相似資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "493a92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要先找全部距離平均值\n",
    "def count_distance(data):\n",
    "    averange = average_count_distance(data)\n",
    "#     尋找之前跟自己距離最近的資料\n",
    "    total_data = data.copy()\n",
    "    total_data = total_data.reset_index(drop=True)\n",
    "    for i in range(len(total_data)):\n",
    "        temp = total_data[0:i+1]\n",
    "        temp = temp[['前1週','前2週','前3週','工作天數','最大溫差']]\n",
    "#         第一筆不找相近的值\n",
    "        if i == 0:\n",
    "            answer = 0\n",
    "#       temp = total_data.loc[i:i,'平均氣溫'].values[0]\n",
    "#             pre_temp = total_data.loc[i-1:i-1,'平均氣溫'].values[0]\n",
    "        else:\n",
    "            find_value = pd.DataFrame(distance_matrix(temp.values, temp.values))\n",
    "#             不要選到0所以把0改成100000\n",
    "            find_value = find_value.replace(0,100000)\n",
    "         \n",
    "#             設定一個閥值，如果沒有大於閥值則使用前一個禮拜，如果有就換成相似資料\n",
    "            find_value_index = find_value.idxmin(axis = 1, skipna = True)[i]\n",
    "            print(\"各筆資料:\",find_value[i][find_value_index])\n",
    "            if find_value[i][find_value_index] > averange:\n",
    "                answer = temp.tail(1)['前3週'].values[0]\n",
    "                print(\"沒有距離小於50\")\n",
    "                print(answer)\n",
    "            else:\n",
    "                print(\"有距離小於40\")\n",
    "                answer = total_data.loc[find_value_index:find_value_index,'數量'].values[0]\n",
    "    \n",
    "        total_data.loc[i:i,'相似資料'] = answer\n",
    "        \n",
    "    return total_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a782d5ed",
   "metadata": {},
   "source": [
    "讀取資料，切割資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e86b688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n",
      "C:\\Users\\iop01\\AppData\\Local\\Temp\\ipykernel_13576\\2613971702.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  part_compute = part_compute.append(last_temp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        庫房         料號    品名     數量        帳務日期   週    工作日     年  工作天數  休假天數\n",
      "67    DLN5  B03106859  人工薄膜   74.0  2019-12-31   1   True  2019   3.0   3.0\n",
      "143   DLN5  B03106859  人工薄膜   95.0  2019-01-13   2  False  2019   5.0   2.0\n",
      "202   DLN5  B03106859  人工薄膜   67.0  2019-01-20   3  False  2019   5.0   2.0\n",
      "261   DLN5  B03106859  人工薄膜   67.0  2019-01-27   4  False  2019   5.0   2.0\n",
      "295   DLN5  B03106859  人工薄膜   37.0  2019-02-03   5   True  2019   5.0   2.0\n",
      "342   DLN5  B03106859  人工薄膜   53.0  2019-02-10   6  False  2019   0.0   7.0\n",
      "421   DLN5  B03106859  人工薄膜   85.0  2019-02-17   7  False  2019   5.0   2.0\n",
      "487   DLN5  B03106859  人工薄膜   80.0  2019-02-24   8  False  2019   5.0   2.0\n",
      "542   DLN5  B03106859  人工薄膜   69.0  2019-03-03   9  False  2019   3.0   4.0\n",
      "622   DLN5  B03106859  人工薄膜   94.0  2019-03-10  10  False  2019   5.0   2.0\n",
      "686   DLN5  B03106859  人工薄膜   78.0  2019-03-17  11  False  2019   5.0   2.0\n",
      "770   DLN5  B03106859  人工薄膜  109.0  2019-03-24  12  False  2019   5.0   2.0\n",
      "828   DLN5  B03106859  人工薄膜   67.0  2019-03-31  13  False  2019   5.0   2.0\n",
      "872   DLN5  B03106859  人工薄膜   54.0  2019-04-07  14  False  2019   3.0   4.0\n",
      "948   DLN5  B03106859  人工薄膜   93.0  2019-04-14  15  False  2019   5.0   2.0\n",
      "1019  DLN5  B03106859  人工薄膜   79.0  2019-04-21  16  False  2019   5.0   2.0\n",
      "1095  DLN5  B03106859  人工薄膜   87.0  2019-04-28  17   True  2019   5.0   2.0\n",
      "1158  DLN5  B03106859  人工薄膜   73.0  2019-05-05  18   True  2019   4.0   3.0\n",
      "1232  DLN5  B03106859  人工薄膜   90.0  2019-05-12  19  False  2019   5.0   2.0\n",
      "1300  DLN5  B03106859  人工薄膜   85.0  2019-05-19  20  False  2019   5.0   2.0\n",
      "1364  DLN5  B03106859  人工薄膜   75.0  2019-05-26  21  False  2019   5.0   2.0\n",
      "1415  DLN5  B03106859  人工薄膜   66.0  2019-06-02  22  False  2019   5.0   2.0\n",
      "1465  DLN5  B03106859  人工薄膜   59.0  2019-06-09  23  False  2019   4.0   3.0\n",
      "1535  DLN5  B03106859  人工薄膜   81.0  2019-06-16  24  False  2019   5.0   2.0\n",
      "1602  DLN5  B03106859  人工薄膜   75.0  2019-06-23  25  False  2019   5.0   2.0\n",
      "1668  DLN5  B03106859  人工薄膜   78.0  2019-06-30  26  False  2019   5.0   2.0\n",
      "1744  DLN5  B03106859  人工薄膜   94.0  2019-07-07  27  False  2019   5.0   2.0\n",
      "1804  DLN5  B03106859  人工薄膜   67.0  2019-07-14  28  False  2019   5.0   2.0\n",
      "1874  DLN5  B03106859  人工薄膜   82.0  2019-07-21  29  False  2019   5.0   2.0\n",
      "1942  DLN5  B03106859  人工薄膜   76.0  2019-07-28  30  False  2019   5.0   2.0\n",
      "1999  DLN5  B03106859  人工薄膜   67.0  2019-08-04  31  False  2019   5.0   2.0\n",
      "2060  DLN5  B03106859  人工薄膜   73.0  2019-08-11  32  False  2019   5.0   2.0\n",
      "2133  DLN5  B03106859  人工薄膜   84.0  2019-08-18  33  False  2019   5.0   2.0\n",
      "2176  DLN5  B03106859  人工薄膜   52.0  2019-08-25  34  False  2019   5.0   2.0\n",
      "2229  DLN5  B03106859  人工薄膜   61.0  2019-09-01  35  False  2019   5.0   2.0\n",
      "2306  DLN5  B03106859  人工薄膜   86.0  2019-09-08  36  False  2019   4.0   3.0\n",
      "2364  DLN5  B03106859  人工薄膜   65.0  2019-09-15  37  False  2019   4.0   3.0\n",
      "2433  DLN5  B03106859  人工薄膜   80.0  2019-09-22  38  False  2019   5.0   2.0\n",
      "2489  DLN5  B03106859  人工薄膜   64.0  2019-09-29  39   True  2019   5.0   2.0\n",
      "2550  DLN5  B03106859  人工薄膜   70.0  2019-10-06  40  False  2019   5.0   2.0\n",
      "2617  DLN5  B03106859  人工薄膜   75.0  2019-10-13  41  False  2019   3.0   4.0\n",
      "2682  DLN5  B03106859  人工薄膜   76.0  2019-10-20  42  False  2019   5.0   2.0\n",
      "2749  DLN5  B03106859  人工薄膜   85.0  2019-10-27  43  False  2019   5.0   2.0\n",
      "2816  DLN5  B03106859  人工薄膜   86.0  2019-11-03  44  False  2019   5.0   2.0\n",
      "2871  DLN5  B03106859  人工薄膜   66.0  2019-11-10  45  False  2019   5.0   2.0\n",
      "2927  DLN5  B03106859  人工薄膜   68.0  2019-11-17  46  False  2019   5.0   2.0\n",
      "2999  DLN5  B03106859  人工薄膜   83.0  2019-11-24  47  False  2019   5.0   2.0\n",
      "3067  DLN5  B03106859  人工薄膜   82.0  2019-12-01  48  False  2019   5.0   2.0\n",
      "3141  DLN5  B03106859  人工薄膜   89.0  2019-12-08  49  False  2019   5.0   2.0\n",
      "3193  DLN5  B03106859  人工薄膜   64.0  2019-12-15  50  False  2019   5.0   2.0\n",
      "3266  DLN5  B03106859  人工薄膜   84.0  2019-12-22  51  False  2019   5.0   2.0\n",
      "3335  DLN5  B03106859  人工薄膜   84.0  2019-12-29  52  False  2019   5.0   2.0\n",
      "3848.0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../使用量計算/週期資料.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 57>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# 把每一週的使用量加總\u001b[39;00m\n\u001b[0;32m     56\u001b[0m data_2019 \u001b[38;5;241m=\u001b[39m week_data_sum(data_2019)\n\u001b[1;32m---> 57\u001b[0m data_2019 \u001b[38;5;241m=\u001b[39m \u001b[43mwork_day\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_2019\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2019\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m data_2020 \u001b[38;5;241m=\u001b[39m week_data_sum(data_2020)\n\u001b[0;32m     59\u001b[0m data_2020 \u001b[38;5;241m=\u001b[39m work_day(data_2020,\u001b[38;5;241m2020\u001b[39m)\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mwork_day\u001b[1;34m(data, year)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwork_day\u001b[39m(data,year):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#     先刪除工作日和休假日\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m工作天數\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m休假天數\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     check_date \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../使用量計算/週期資料.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     answer \u001b[38;5;241m=\u001b[39m  pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m      7\u001b[0m     date \u001b[38;5;241m=\u001b[39m  pd\u001b[38;5;241m.\u001b[39mDataFrame()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Medical_Materials\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Medical_Materials\\lib\\site-packages\\pandas\\util\\_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    312\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    313\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    314\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    315\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(inspect\u001b[38;5;241m.\u001b[39mcurrentframe()),\n\u001b[0;32m    316\u001b[0m     )\n\u001b[1;32m--> 317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Medical_Materials\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Medical_Materials\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Medical_Materials\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Medical_Materials\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1729\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1727\u001b[0m     is_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1728\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1740\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Medical_Materials\\lib\\site-packages\\pandas\\io\\common.py:857\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    856\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    866\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../使用量計算/週期資料.csv'"
     ]
    }
   ],
   "source": [
    "# B03106859\n",
    "# C13930399\n",
    "# A00120213\n",
    "# A02120180\n",
    "# A02320340\n",
    "# A02322259\n",
    "# A04411285\n",
    "# A04800036\n",
    "# B00206057 50週\n",
    "# B03110120\n",
    "# G81500272\n",
    "# K80004044\n",
    "# A00120033合併\n",
    "# A00120067合併\n",
    "# A00120110合併\n",
    "# A04200004合併\n",
    "\n",
    "\n",
    "\n",
    "part_no ='B03106859'\n",
    "room = '五病房'\n",
    "\n",
    "\n",
    "# 要提取前幾天的資料\n",
    "before_n = 4\n",
    "# 提取相關性資料筆數\n",
    "take_num = 4\n",
    "# 平滑參數\n",
    "smooth_num = 2\n",
    "# 提取前n筆的平均值(滑動平均)\n",
    "window_size = 3\n",
    "\n",
    "# 需要投入的欄位\n",
    "# columns = ['年','工作天數','休假天數','前1週','前2週','前3週','前4週','第1週','第2週','第3週','第4週']\n",
    "# columns = ['年','工作天數','休假天數','前1週','前2週','前3週','前4週','每月第幾週',corr_data_name[0],corr_data_name[1],corr_data_name[2]]\n",
    "# columns = ['前2週','每月第幾週','總下雨量','平均濕度','休假天數','工作天數']\n",
    "# columns = ['工作天數','休假天數','前1週','前2週','前3週','前4週','每月第幾週','平均氣溫','滑動平均']\n",
    "columns = ['工作天數','休假天數','年','前1週','前2週','前3週','前4週','每月第幾週','平均氣溫','滑動平均']\n",
    "columns = ['週','每月第幾週','前1週','前2週','前3週','休假天數','最大溫差','相似資料','前1週開刀人數']\n",
    "SMOGN_columns = ['工作天數','休假天數','年','前1週','前2週','前3週','前4週','每月第幾週','平均氣溫','滑動平均','數量']\n",
    "\n",
    "# columns = ['前1週','前2週','前3週','前4週','年','週']\n",
    "\n",
    "\n",
    "data=pd.read_csv(f\"../使用量計算全部資料code/有加工作日/{room}/{part_no}.csv\")\n",
    "data = data[data.columns.drop(list(data.filter(regex='Unnamed')))]\n",
    "# data['年'] =  pd.to_datetime(data['帳務日期']).dt.year\n",
    "\n",
    "data_2019 = data[data['年']==2019]\n",
    "data_2020 = data[data['年']==2020]\n",
    "data_2021 = data[data['年']==2021]\n",
    "data_2022 = data[data['年']==2022]\n",
    "\n",
    "\n",
    "# 把每一週的使用量加總\n",
    "data_2019 = week_data_sum(data_2019)\n",
    "data_2019 = work_day(data_2019,2019)\n",
    "data_2020 = week_data_sum(data_2020)\n",
    "data_2020 = work_day(data_2020,2020)\n",
    "data_2021 = week_data_sum(data_2021)\n",
    "data_2021 = work_day(data_2021,2021)\n",
    "data_2022 = week_data_sum(data_2022)\n",
    "data_2022 = work_day(data_2022,2022)\n",
    "print(\"2022資料\",data_2022)\n",
    "\n",
    "total_data = pd.concat([data_2019, data_2020])\n",
    "total_data = pd.concat([total_data, data_2021])\n",
    "total_data = pd.concat([total_data, data_2022])\n",
    "\n",
    "total_data = total_data.reset_index()\n",
    "\n",
    "\n",
    "# 提取相似日\n",
    "total_data =  similar_day(total_data)\n",
    "\n",
    "\n",
    "# 新增相關性衛材資料取3筆，由於計算的歷史資料到前4個禮拜因此刪除前4筆資料\n",
    "# corr_data = count_corr(part_no,take_num)\n",
    "# # corr_data=corr_data.drop(corr_data.index[range(0,before_n,1)]).reset_index()\n",
    "# corr_data = corr_data[corr_data.columns.drop(list(corr_data.filter(regex='index')))]\n",
    "# corr_data_name = corr_data.columns.tolist()\n",
    "# # features = features + corr_data_name\n",
    "# # 和原本的data合併\n",
    "# total_data = pd.concat([total_data, corr_data ], axis = 1)\n",
    "\n",
    "# # 新增降水量資料&是否有下雨&溫度&濕度\n",
    "# total_data = weather_data(total_data)\n",
    "# print(\"2022資料\",total_data)\n",
    "\n",
    "#新增溫度差資料\n",
    "# total_data = Temperature_difference(total_data)\n",
    "\n",
    "# Temperature = total_data\n",
    "total_data = MaxTemperature_difference(total_data)\n",
    "\n",
    "\n",
    "\n",
    "# 針對過年資料作處理\n",
    "# total_data['使用量(過年)'] = total_data['前1週']\n",
    "total_data,new_year_day = new_year(total_data)\n",
    "\n",
    "# 算出前n筆的平均值\n",
    "total_data = before_n_mean(total_data,window_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 提取前幾天資料\n",
    "total_data = total_data.reset_index()\n",
    "total_data = take_data(total_data,before_n)\n",
    "total_data.drop(total_data.head(before_n).index,inplace=True) # 从头去掉n行\n",
    "total_data['帳務日期'] =  pd.to_datetime(total_data['帳務日期'])\n",
    "\n",
    "# 找出相似的資料\n",
    "total_data =  count_distance(total_data)\n",
    "print(\"相似的資料\",total_data)\n",
    "\n",
    "# 增加流感資料\n",
    "# total_data = theflu_sum(total_data)\n",
    "\n",
    "# 增加預測病人人數資料\n",
    "# total_data = pred_people(total_data)\n",
    "\n",
    "# 對歷史資料做差分\n",
    "# total_data['前1週'] = total_data['前1週'].diff(1)\n",
    "# total_data['前2週'] = total_data['前2週'].diff(1)\n",
    "\n",
    "# total_data.drop(total_data.head(8).index,inplace=True) # 从头去掉n行\n",
    "\n",
    "# 平滑處理\n",
    "# total_data = smooth(total_data,smooth_num)\n",
    "\n",
    "\n",
    "# 新增該資料為每月的第幾週\n",
    "total_data['每月第幾週'] = total_data['週'] % 4\n",
    "\n",
    "\n",
    "\n",
    "# 新增病房人數\n",
    "total_data = people_data(total_data)\n",
    "# 新增開刀人數\n",
    "total_data = surgery_data(total_data)\n",
    "print(\"開刀總人數\",total_data[\"開刀總人數\"])\n",
    "\n",
    "# 按年、週排序\n",
    "total_data = total_data.sort_values(['年'], ascending=True).reset_index(drop=True)\n",
    "total_data = total_data.sort_values(['週'], ascending=True).reset_index(drop=True)\n",
    "print(\"查看資料2022\",total_data[total_data['年']== 2022])\n",
    "\n",
    "# 每月第幾週做one_hot\n",
    "# total_data['每月第幾週'] = total_data['每月第幾週'].apply(lambda x: detect_week(x)) \n",
    "# temp = pd.get_dummies(total_data['每月第幾週'])\n",
    "# total_data = pd.concat([total_data, temp], axis = 1)\n",
    "\n",
    "\n",
    "total_data['分類年'] = total_data['年']\n",
    "# 訓練資料\n",
    "train_data =  total_data[(total_data['分類年']== 2019)|(total_data['分類年']== 2020)|(total_data['分類年']== 2021)]    \n",
    "# train_data =  total_data[total_data['分類年']== 2019] \n",
    "\n",
    "# 異常值排除\n",
    "# print(\"排除前長度\",len(total_data))\n",
    "# total_data = Iqr_outlier(total_data,train_data)\n",
    "# print(\"排除後長度\",len(total_data))\n",
    "\n",
    "# SMOGN擴增資料\n",
    "# train_data = SMOGN(train_data[SMOGN_columns])\n",
    "# temp_train_data = train_data\n",
    "\n",
    "# 新增資料分類的特徵\n",
    "# k_mean_data = pd.DataFrame()\n",
    "# k_mean_data = k_mean(total_data[columns])\n",
    "# # 和total_data合併\n",
    "# total_data['資料分類'] = k_mean_data\n",
    "# # total_data = data_normalize(total_data, [\"資料分類\"])\n",
    "# columns = columns + [\"資料分類\"]\n",
    "\n",
    "# train_data =  total_data[total_data['年']== 2019] \n",
    "# 正規化\n",
    "train_data =  total_data[(total_data['分類年']== 2019)|(total_data['分類年']== 2020)|(total_data['分類年']== 2021)] \n",
    "target_min, target_max = train_data['數量'].min(), train_data['數量'].max()\n",
    "target_mean,target_std = train_data['數量'].mean(), train_data['數量'].std()\n",
    "target_median,target_first,target_third =train_data['數量'].quantile(0.5),train_data['數量'].quantile(0.25),train_data['數量'].quantile(0.75)\n",
    "print(\"最大、最小值:\",target_min,target_max)\n",
    "\n",
    "\n",
    "\n",
    "# need_normalize = need_normalize + corr_data_name\n",
    "\n",
    "# 最大最小值正規化\n",
    "total_data = data_normalize(total_data, need_normalize)\n",
    "\n",
    "# Z-score正規化\n",
    "# total_data = zscore_normalize(total_data, robust_need_normalize)\n",
    "# 羅伯斯特正規化robust_normalize\n",
    "# total_data = robust_normalize(total_data, robust_need_normalize)\n",
    "\n",
    "# 擴增資料做正規化\n",
    "# train_data = data_normalize(train_data, SMOGN_columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 做完正規畫後從新給予訓練資料\n",
    "train_data =  total_data[(total_data['分類年']== 2019)|(total_data['分類年']== 2020)|(total_data['分類年']== 2021)]    \n",
    "# train_data =  total_data[total_data['分類年']== 2019] \n",
    "\n",
    "# train_data = smooth(train_data,smooth_num)\n",
    "# x_train = train_data[features]\n",
    "\n",
    "# 透過F檢定後重新給予N個特徵\n",
    "y_train = train_data[\"數量\"]\n",
    "# columns = feature_point(features,x_train,y_train,8)\n",
    "# columns = columns + [\"資料分類\"]\n",
    "\n",
    "x_train = train_data[columns]\n",
    "\n",
    "#測試資料\n",
    "test_data = total_data[total_data['分類年']== 2022]     \n",
    "x_test = test_data[columns]\n",
    "y_test = test_data[\"數量\"]\n",
    "print(\"資料檢查\",y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091ca8d3",
   "metadata": {},
   "source": [
    "建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca9e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 網格調參\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# other_params = {'learning_rate': 0.01, 'n_estimators': 668, 'gamma': 0.01, 'max_depth': 2, 'min_child_weight': 1,\n",
    "#                 'colsample_bytree': 0.1, 'colsample_bylevel': 1, 'subsample': 0.1, 'reg_lambda': 1, 'reg_alpha': 0,\n",
    "#                 'seed': 1}\n",
    "# cv_params = {'learning_rate': np.logspace(-2, 0, 10)}\n",
    "# regress_model = xgb.XGBRegressor(**other_params)  # 注意这里的两个 * 号！\n",
    "# gs = GridSearchCV(regress_model, cv_params, verbose=2, refit=True, cv=5, n_jobs=-1)\n",
    "# gs.fit(x_train, y_train)  # X为训练数据的特征值，y为训练数据的label\n",
    "# # 性能测评\n",
    "# print(\"参数的最佳取值：:\", gs.best_params_)\n",
    "# print(\"最佳模型得分:\", gs.best_score_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 建立 XGBRegressor 模型\n",
    "xgbrModel = xgb.XGBRegressor(learning_rate=0.01, \n",
    "                        gamma = 0.01, \n",
    "                        max_depth=2,\n",
    "                        colsample_bytree=0.1,\n",
    "                        reg_lambda=0.01,\n",
    "                        seed=1,\n",
    "                        subsample=0.1,\n",
    "                        min_child_weight=1,\n",
    "                        n_estimators=668)\n",
    "# xgbrModel = xgb.XGBRegressor()\n",
    "\n",
    "# 使用訓練資料訓練模型\n",
    "xgbrModel.fit(x_train,y_train)\n",
    "\n",
    "# 儲存模型\n",
    "# joblib.dump(xgbrModel,f'../自動化系統/XGB模型/五病房/{part_no}.pkl')\n",
    "# 使用訓練資料預測\n",
    "y_pred=xgbrModel.predict(x_test)\n",
    "\n",
    "# # SVR\n",
    "# SVRModel=svm.SVR(C=2, kernel=\"rbf\", gamma='auto')\n",
    "\n",
    "# #     # 使用訓練資料訓練模型\n",
    "# SVRModel.fit(x_train,y_train)\n",
    "# y_pred=SVRModel.predict(x_test)\n",
    "# # 儲存模型\n",
    "# joblib.dump(SVRModel,f'../自動化系統/SVR模型/五病房/{part_no}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b60e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c2585",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6006b51b",
   "metadata": {},
   "source": [
    "評估績效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c12437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def nMAE(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs(y_true - y_pred))/y_true.mean() * 100\n",
    "\n",
    "def RMSE(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.sqrt(((y_pred - y_true) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac788f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_year_day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f9cf9b",
   "metadata": {},
   "source": [
    "將過年刪除的資料放回test資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee3584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_data_back(y_test,y_pred,new_year_day,test_year):\n",
    "\n",
    "    y_test_copy = y_test.copy()\n",
    "    y_pred = pd.DataFrame(y_pred,columns=['數量'])\n",
    "    y_pred_copy = y_pred.copy()\n",
    "#     算出他維資料集中的第幾筆\n",
    "    new_year_day = new_year_day[new_year_day['年'] == 2022]\n",
    "    y_test_copy = y_test_copy.reset_index()\n",
    "    y_test_copy = y_test_copy[y_test_copy.columns.drop(list(y_test_copy.filter(regex='index')))]\n",
    "#     插入被刪除過年資料(y_test)\n",
    "    df_part_1 = y_test_copy.loc[0:3]\n",
    "    df_part_2 = y_test_copy.loc[4:]\n",
    "#     print(df_part_2)\n",
    "    y_test = df_part_1['數量'].append(new_year_day['數量'],ignore_index = True)\n",
    "    y_test = y_test.append(df_part_2['數量'],ignore_index = True)\n",
    "  #     插入被刪除過年資料(y_pred)\n",
    "    df_part_1_pred = y_pred_copy.loc[0:3]\n",
    "    df_part_2_pred = y_pred_copy.loc[4:]\n",
    "    \n",
    "    y_pred = df_part_1_pred['數量'].append(new_year_day['使用量(過年)'],ignore_index = True)\n",
    "    y_pred = y_pred.append(df_part_2_pred['數量'],ignore_index = True)\n",
    "    return y_test,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c409e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_year_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7252c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = new_year_day[new_year_day['年'] == 2022]\n",
    "temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82c12b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # 最大最小值反正規化\n",
    "y_test = y_test * (target_max - target_min) + target_min\n",
    "y_pred = y_pred * (target_max - target_min) + target_min\n",
    "# z-score反正規化\n",
    "# y_test = y_test * target_std +  target_mean\n",
    "# y_pred = y_pred * target_std +  target_mean\n",
    "# # 羅伯斯特正規化\n",
    "# y_test = y_test * target_std +  target_mean\n",
    "# y_pred = y_pred * (target_third - target_first) + target_median\n",
    "\n",
    "y_test,y_pred = put_data_back(y_test,y_pred,new_year_day,2022)\n",
    "\n",
    "mape = round(MAPE(y_test, y_pred),2)\n",
    "rmse = round(RMSE(y_test, y_pred),2)\n",
    "mae = round(nMAE(y_test, y_pred),2)\n",
    "pred_result = pd.DataFrame({'料號': part_no,'P(RMSE)': rmse, 'P(MAPE)': mape,'P(Mae)': mae},index=[0])\n",
    "pred_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2940da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01535373",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "line_color = [\n",
    "    '#1f77b4',  # muted blue\n",
    "    '#ff7f0e',  # safety orange\n",
    "    '#2ca02c',  # cooked asparagus green\n",
    "    '#d62728',  # brick red\n",
    "    '#9467bd',  # muted purple\n",
    "    '#8c564b',  # chestnut brown\n",
    "    '#e377c2',  # raspberry yogurt pink\n",
    "    '#7f7f7f',  # middle gray\n",
    "    '#bcbd22',  # curry yellow-green\n",
    "    '#17becf'   # blue-teal\n",
    "]\n",
    "\n",
    "My_list = [*range(1, 53, 1)]\n",
    "\n",
    "\n",
    "data=pd.read_csv(f\"../使用量計算/有加工作日/{room}/{part_no}.csv\")\n",
    "data = data[data.columns.drop(list(data.filter(regex='Unnamed')))]\n",
    "# data['年'] =  pd.to_datetime(data['帳務日期']).dt.year\n",
    "\n",
    "data_2019 = data[data['年']==2019]\n",
    "data_2020 = data[data['年']==2020]\n",
    "data_2021 = data[data['年']==2021]\n",
    "data_2022 = data[data['年']==2022]\n",
    "# 把每一週的使用量加總\n",
    "data_2019 = week_data_sum(data_2019)\n",
    "data_2020 = week_data_sum(data_2020)\n",
    "data_2021 = week_data_sum(data_2021)\n",
    "data_2022 = week_data_sum(data_2022)\n",
    "total_data_real = pd.concat([data_2019, data_2020])\n",
    "total_data_real = pd.concat([total_data_real, data_2021])\n",
    "total_data_real = pd.concat([total_data_real, data_2022])\n",
    "total_data_real = total_data_real.reset_index()\n",
    "\n",
    "data_2019_1 =  total_data_real[total_data_real['年'] == 2019]\n",
    "data_2020_1 =  total_data_real[total_data_real['年'] == 2020]\n",
    "data_2021_1 =  total_data_real[total_data_real['年'] == 2021]\n",
    "\n",
    "# temp = Temperature[Temperature['年'] == 2021]\n",
    "# temp = temp['平均溫度差']\n",
    "\n",
    "fig_line = go.Figure()\n",
    "fig_line.add_trace(go.Scatter(y =data_2021['數量'] , x=My_list,\n",
    "                    mode='lines',\n",
    "                    name='2021',\n",
    "                    line={'dash': 'dash'},\n",
    "                    line_color= '#1f77b4'))\n",
    "\n",
    "fig_line.add_trace(go.Scatter(y = y_pred, x=My_list,\n",
    "                    mode='lines',\n",
    "                    name='預測',\n",
    "                    line_color= '#ff7f0e'))\n",
    "\n",
    "fig_line.add_trace(go.Scatter(y = y_test, x=My_list,\n",
    "                    mode='lines',\n",
    "                    name='2021',\n",
    "                    line_color= '#ff0e16'))\n",
    "# fig_line.add_trace(go.Scatter(y =  data_2020['數量'], x=My_list,\n",
    "#                     mode='lines',\n",
    "#                     name='2020',\n",
    "#                     line_color= '#17becf'))\n",
    "# fig_line.add_trace(go.Scatter(y = data_2019['數量'], x=My_list,\n",
    "#                     mode='lines',\n",
    "#                     name='2019',\n",
    "#                     line_color= '#bcbd22'))\n",
    "\n",
    "\n",
    "fig_line.update_layout(\n",
    "    yaxis_title='使用量',\n",
    "    xaxis_title='日期',\n",
    "    title='用量',\n",
    "    font=dict(\n",
    "        size=18,\n",
    "    ),\n",
    "#     yaxis2=dict(anchor='x', overlaying='y', side='right')\n",
    "    height=450, \n",
    "    width=1500,\n",
    "\n",
    ")\n",
    "\n",
    "fig_line.update_xaxes(nticks=52)\n",
    "\n",
    "\n",
    "#     fig_line.write_html(f'{folder_path}/img/{methods}_{i}.html')\n",
    "\n",
    "fig_line.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d76963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f23f641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
